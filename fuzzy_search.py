"""
arxiv_semantic_search.py
ç¤ºä¾‹ä¾èµ–:
    pip install "transformers>=4.51.0" faiss-cpu torch datasets tqdm
"""

import json, os, faiss, torch, torch.nn.functional as F
from tqdm import tqdm
from pathlib import Path
from typing import List, Tuple
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM

# --------------------------- 1. æ¨¡å‹ä¸è¶…å‚ ---------------------------
EMB_MODEL_NAME   = "Qwen/Qwen3-Embedding-4B"
RERANK_MODEL     = "Qwen/Qwen3-Reranker-4B"   # è‹¥ä¸ç”¨ç²¾æ’ï¼Œå¯ä¸åŠ è½½
DEVICE           = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LEN          = 2048                         # æ ¹æ®æ˜¾å­˜å¯è°ƒ
TOP_K_RECALL     = 100                          # ANN å¬å›æ•°
TOP_K_FINAL      = 10                           # æœ€ç»ˆå±•ç¤ºç»™ç”¨æˆ·çš„æ•°ç›®
INDEX_FILE       = "arxiv_demo.index"
DOC_STORE_FILE   = "arxiv_docs.jsonl"           # å­˜æ–‡æ¡£å…ƒä¿¡æ¯ï¼Œæ–¹ä¾¿å±•ç¤º

# --------------------------- 2. å·¥å…·å‡½æ•° ---------------------------
def last_token_pool(last_hidden, attn_mask):
    """å®˜æ–¹æ¨èçš„ embedding poolingï¼šå–æ¯æ¡åºåˆ—æœ€åä¸€ä¸ªé padding token"""
    seq_len = attn_mask.sum(dim=1) - 1
    return last_hidden[torch.arange(last_hidden.size(0)), seq_len]

def embed_texts(texts: List[str], tokenizer, model) -> torch.Tensor:
    tok = tokenizer(texts, truncation=True, padding=True,
                    max_length=MAX_LEN, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        out = model(**tok).last_hidden_state
    vec = last_token_pool(out, tok["attention_mask"])
    return F.normalize(vec, p=2, dim=1)

def build_instruction_query(raw_query: str) -> str:
    instr = "Given a web search query, retrieve relevant passages that answer the query"
    return f"Instruct: {instr}\nQuery: {raw_query}"

def rerank(
    query: str,
    cand_indices: List[int],
    docs: List[dict],
    tokenizer_rr,
    model_rr
) -> List[Tuple[int, float]]:
    """å¯¹å¬å›çš„æ–‡æ¡£åšç²¾æ’ï¼Œè¿”å› [(doc_id, score)]"""
    pairs = []
    for idx in cand_indices:
        doc = docs[idx]["text"]
        pairs.append(f"<Instruct>: Given a web search query, retrieve relevant passages that answer the query\n"
                     f"<Query>: {query}\n"
                     f"<Document>: {doc}")
    # æ‰¹é‡æ¨ç†
    tok = tokenizer_rr(pairs, padding=True, truncation=True,
                       max_length=MAX_LEN, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        logits = model_rr(**tok).logits[:, -1, :]
    yes_id = tokenizer_rr.convert_tokens_to_ids("yes")
    no_id  = tokenizer_rr.convert_tokens_to_ids("no")
    scores = torch.softmax(logits[:, [no_id, yes_id]], dim=1)[:, 1]  # å– "yes" æ¦‚ç‡
    return sorted(zip(cand_indices, scores.tolist()),
                  key=lambda x: x[1], reverse=True)

# --------------------------- 3. ç¦»çº¿é˜¶æ®µï¼šæ„å»ºå‘é‡åº“ ---------------------------
def offline_build():
    print("â–¶ ä¸‹è½½/è¯»å–ç¤ºä¾‹æ•°æ®ï¼ˆ100 ç¯‡è®ºæ–‡ï¼‰")
    ds = load_dataset("ccdv/arxiv-classification", split="train[:100]")
    # æ–‡æ¡£æ ¼å¼ï¼šTitle + Abstract
    docs_jsonl = []
    texts = []
    for row in ds:
        text = f"Title: {row['title']}\nAbstract: {row['abstract']}"
        texts.append(text)
        docs_jsonl.append({"id": len(docs_jsonl), "title": row["title"], "text": text})

    print("â–¶ åŠ è½½ Qwen3-Embedding æ¨¡å‹")
    tok_emb = AutoTokenizer.from_pretrained(EMB_MODEL_NAME, padding_side="left")
    model_emb = AutoModel.from_pretrained(EMB_MODEL_NAME).to(DEVICE).eval()

    print("â–¶ è®¡ç®—æ–‡æ¡£ embedding")
    embeddings = []
    batch_size = 8
    for i in tqdm(range(0, len(texts), batch_size)):
        batch_vec = embed_texts(texts[i:i+batch_size], tok_emb, model_emb)
        embeddings.append(batch_vec.cpu())
    embeddings = torch.cat(embeddings).numpy().astype("float32")

    print("â–¶ æ„å»º Faiss å‘é‡ç´¢å¼•")
    index = faiss.IndexFlatIP(embeddings.shape[1])       # å†…ç§¯ = å½’ä¸€åŒ–å cosine
    index.add(embeddings)
    faiss.write_index(index, INDEX_FILE)
    with open(DOC_STORE_FILE, "w") as f:
        for obj in docs_jsonl:
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")

    print(f"âœ… ç´¢å¼•å®Œæˆï¼š{len(docs_jsonl)} ç¯‡æ–‡æ¡£, index => {INDEX_FILE}")

# --------------------------- 4. åœ¨çº¿é˜¶æ®µï¼šæŸ¥è¯¢ ---------------------------
def online_search():
    if not Path(INDEX_FILE).exists():
        print("â— è¯·å…ˆè¿è¡Œ offline_build() å»ºåº“")
        return

    print("â–¶ åŠ è½½ç´¢å¼• & æ–‡æ¡£å…ƒä¿¡æ¯")
    index = faiss.read_index(INDEX_FILE)
    docs  = [json.loads(line) for line in open(DOC_STORE_FILE, encoding="utf-8")]
    tok_emb = AutoTokenizer.from_pretrained(EMB_MODEL_NAME, padding_side="left")
    model_emb = AutoModel.from_pretrained(EMB_MODEL_NAME).to(DEVICE).eval()

    # âš ï¸ è‹¥è¦å¯ç”¨ç²¾æ’ï¼š
    use_rerank = True
    if use_rerank:
        tok_rr  = AutoTokenizer.from_pretrained(RERANK_MODEL, padding_side="left")
        model_rr = AutoModelForCausalLM.from_pretrained(RERANK_MODEL).to(DEVICE).eval()

    while True:
        raw_query = input("\nâ“ è¾“å…¥æ£€ç´¢è¯­å¥(å›è½¦é€€å‡º)ï¼š").strip()
        if not raw_query:
            break

        q_text = build_instruction_query(raw_query)
        q_vec  = embed_texts([q_text], tok_emb, model_emb).cpu().numpy().astype("float32")

        D, I = index.search(q_vec, TOP_K_RECALL)   # I: indices  D: similarity
        cand_indices = I[0].tolist()

        if use_rerank:
            reranked = rerank(raw_query, cand_indices, docs, tok_rr, model_rr)[:TOP_K_FINAL]
        else:
            reranked = list(zip(cand_indices, D[0]))[:TOP_K_FINAL]

        print("\nğŸ” Top ç»“æœï¼š")
        for rank, (doc_id, score) in enumerate(reranked, 1):
            paper = docs[doc_id]
            title = paper["title"][:120].replace("\n", " ")
            print(f"{rank:02d}. [{score:.3f}] {title}")

# --------------------------- 5. å…¥å£ ---------------------------
if __name__ == "__main__":
    if not Path(INDEX_FILE).exists():
        offline_build()   # ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨å»ºåº“ï¼ˆçº¦ 2~3 åˆ†é’Ÿ/CPUï¼‰
    online_search()
